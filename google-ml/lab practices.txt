qwiklabs-gcp-7213f420216c9afb


- To disk
- Note down the name of your bucket: qwiklabs-gcp-7213f420216c9afb
- What is the URL of the published Cloud Storage file? https://storage.googleapis.com/qwiklabs-gcp-7213f420216c9afb/earthquakes/earthquakes.htm
- What are some advantages of publishing to Cloud Storage? We don't have to worry about that data will be removed after shut down VM instance.
- Does deleting the instance have any impact on the files that you stored on Cloud Storage? No




cat /proc/cpuinfo

sudo apt-get update
sudo apt-get -y -qq install git

git --version

git clone https://github.com/GoogleCloudPlatform/training-data-analyst

cd training-data-analyst/courses/machine_learning/deepdive/01_googleml/earthquakes

less ingest.sh

bash ingest.sh

head earthquakes.csv

https://github.com/GoogleCloudPlatform/datalab-samples/blob/master/basemap/earthquakes.ipynb

bash install_missing.sh

./transform.py

ls -l

gsutil cp earthquakes.* gs://qwiklabs-gcp-7213f420216c9afb/earthquakes/

gsutil acl ch -u AllUsers:R gs://qwiklabs-gcp-7213f420216c9afb/earthquakes/*



==================================
Big Query

qwiklabs-gcp-4503425b33f0d032

gcloud compute zones list

datalab create mydatalabvm --zone us-east1-c

- Click on Web Preview icon on the top-right corner of the Cloud Shell ribbon. Click Change Port and enter the port 8081 and click Change and Preview.

- https://console.cloud.google.com/apis/library/sourcerepo.googleapis.com/?q=Repositories

- https://8081-dot-7202347-dot-devshell.appspot.com/tree/datalab

#standardSQL
SELECT
  departure_delay,
  COUNT(1) AS num_flights,
  APPROX_QUANTILES(arrival_delay, 5) AS arrival_delay_quantiles
FROM
  `bigquery-samples.airline_ontime_data.flights`
GROUP BY
  departure_delay
HAVING
  num_flights > 100
ORDER BY

#standardSQL
SELECT
  departure_airport,
  arrival_airport,
  COUNT(1) AS num_flights
FROM
  `bigquery-samples.airline_ontime_data.flights`
GROUP BY
  departure_airport,
  arrival_airport
ORDER BY
  num_flights DESC
LIMIT
  10
  departure_delay ASC


query="""
SELECT
  departure_delay,
  COUNT(1) AS num_flights,
  APPROX_QUANTILES(arrival_delay, 10) AS arrival_delay_deciles
FROM
  `bigquery-samples.airline_ontime_data.flights`
GROUP BY
  departure_delay
HAVING
  num_flights > 100
ORDER BY
  departure_delay ASC
"""

import google.datalab.bigquery as bq
df = bq.Query(query).execute().result().to_dataframe()
df.head()


import pandas as pd
percentiles = df['arrival_delay_deciles'].apply(pd.Series)
percentiles = percentiles.rename(columns = lambda x : str(x*10) + "%")
df = pd.concat([df['departure_delay'], percentiles], axis=1)
df.head()


without_extremes = df.drop(['0%', '100%'], 1)
without_extremes.plot(x='departure_delay', xlim=(-30,50), ylim=(-50,50));


datalab connect mydatalabvm

===================================================================
Invoking Machine Learning APIs

- 8081
- repocheckout

%bash
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
rm -rf training-data-analyst/.git

- ensure you see the training-data-analyst

- In the Datalab browser, navigate to training-data-analyst > courses > machine_learning > deepdive > 01_googleml > mlapis.ipynb

===================================================================
Lab: Creating Repeatable Dataset Splits in BigQuery & Lab: Exploring and Creating ML Datasets

- 8081

in notebook
- Rename this notebook as repocheckout

%bash
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
rm -rf training-data-analyst/.git

- datalab > notebooks > training-data-analyst > courses > machine_learning > deepdive > 02_generalization and open repeatable_splitting.ipynb

- (if disconnected) datalab connect mydatalabvm

query = """
SELECT
  pickup_datetime,
  pickup_longitude, pickup_latitude, 
  dropoff_longitude, dropoff_latitude,
  passenger_count,
  trip_distance,
  tolls_amount,
  fare_amount,
  total_amount
FROM 
  `nyc-tlc.yellow.trips`
WHERE
  MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 100000) = 0
"""
trips = bq.Query(query).execute().result().to_dataframe()


[Course 3 - Intro to TensorFlow] ===================================================================
Lab: [ML on GCP C3] Implementing a Machine Learning model in TensorFlow using Estimator API

- Rename this notebook as repocheckout.
%bash
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
rm -rf training-data-analyst/.git

- In Cloud Datalab, click on the Home icon, and then navigate to datalab > notebooks > training-data-analyst > courses > machine_learning > deepdive > 03_tensorflow > labs and open b_estimator.ipynb.

import tensorflow as tf
import pandas as pd
import numpy as np
import shutil

print(tf.__version__)


# In CSV, label is the first column, after the features, followed by the key
CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']
FEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]
LABEL = CSV_COLUMNS[0]

df_train = pd.read_csv('./taxi-train.csv', header = None, names = CSV_COLUMNS)
df_valid = pd.read_csv('./taxi-valid.csv', header = None, names = CSV_COLUMNS)
df_test = pd.read_csv('./taxi-test.csv', header = None, names = CSV_COLUMNS)


def make_train_input_fn(df, num_epochs):
  return tf.estimator.inputs.pandas_input_fn(
    x = df,
    y = df[LABEL],
    batch_size = 128,
    num_epochs = num_epochs,
    shuffle = True,
    queue_capacity = 1000
  )
  
  
def make_eval_input_fn(df):
  return tf.estimator.inputs.pandas_input_fn(
    x = df,
    y = df[LABEL],
    batch_size = 128,
    shuffle = True,
    queue_capacity = 1000
  )


def make_prediction_input_fn(df):
  return tf.estimator.inputs.pandas_input_fn(
    x = df,
    y = None,
    batch_size = 128,
    shuffle = True,
    queue_capacity = 1000
  )
  

def create_feature_cols():
  return [tf.feature_column.numeric_column(k) for k in FEATURES]


tf.logging.set_verbosity(tf.logging.INFO)

OUTDIR = 'taxi_trained'
shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time

# TODO: Train a linear regression model
model = tf.estimator.LinearRegressor(feature_columns = create_feature_cols(), model_dir = OUTDIR)

model.train(input_fn = make_train_input_fn(df_train, num_epochs = 10))


def print_rmse(model, df):
  metrics = model.evaluate(input_fn = make_eval_input_fn(df))
  print('RMSE on dataset = {}'.format(np.sqrt(metrics['average_loss'])))
print_rmse(model, df_valid)


predictions = model.predict(input_fn = make_prediction_input_fn(df_test))
for p in predictions:
  print(p)


tf.estimator.DNNClassifier(feature_columns = create_feature_cols(), hidden_units = [32, 8, 2], model_dir=OUTDIR)
model.train(input_fn = make_train_input_fn(df_train, num_epochs = 50))


import google.datalab.bigquery as bq
import numpy as np
import pandas as pd

def create_query(phase, EVERY_N):
  """
  phase: 1 = train 2 = valid
  """
  base_query = """
SELECT
  (tolls_amount + fare_amount) AS fare_amount,
  EXTRACT(DAYOFWEEK FROM pickup_datetime) * 1.0 AS dayofweek,
  EXTRACT(HOUR FROM pickup_datetime) * 1.0 AS hourofday,
  pickup_longitude AS pickuplon,
  pickup_latitude AS pickuplat,
  dropoff_longitude AS dropofflon,
  dropoff_latitude AS dropofflat,
  passenger_count * 1.0 AS passengers,
  CONCAT(CAST(pickup_datetime AS STRING), CAST(pickup_longitude AS STRING), CAST(pickup_latitude AS STRING), CAST(dropoff_latitude AS STRING), CAST(dropoff_longitude AS STRING)) AS key
FROM
  `nyc-tlc.yellow.trips`
WHERE
  trip_distance > 0
  AND fare_amount >= 2.5
  AND pickup_longitude > -78
  AND pickup_longitude < -70
  AND dropoff_longitude > -78
  AND dropoff_longitude < -70
  AND pickup_latitude > 37
  AND pickup_latitude < 45
  AND dropoff_latitude > 37
  AND dropoff_latitude < 45
  AND passenger_count > 0
  """

  if EVERY_N == None:
    if phase < 2:
      # Training
      query = "{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 4) < 2".format(base_query)
    else:
      # Validation
      query = "{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 4) = {1}".format(base_query, phase)
  else:
    query = "{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), {1}) = {2}".format(base_query, EVERY_N, phase)
    
  return query

query = create_query(2, 100000)
df = bq.Query(query).execute().result().to_dataframe()

