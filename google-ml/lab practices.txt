qwiklabs-gcp-7213f420216c9afb


- To disk
- Note down the name of your bucket: qwiklabs-gcp-7213f420216c9afb
- What is the URL of the published Cloud Storage file? https://storage.googleapis.com/qwiklabs-gcp-7213f420216c9afb/earthquakes/earthquakes.htm
- What are some advantages of publishing to Cloud Storage? We don't have to worry about that data will be removed after shut down VM instance.
- Does deleting the instance have any impact on the files that you stored on Cloud Storage? No




cat /proc/cpuinfo

sudo apt-get update
sudo apt-get -y -qq install git

git --version

git clone https://github.com/GoogleCloudPlatform/training-data-analyst

cd training-data-analyst/courses/machine_learning/deepdive/01_googleml/earthquakes

less ingest.sh

bash ingest.sh

head earthquakes.csv

https://github.com/GoogleCloudPlatform/datalab-samples/blob/master/basemap/earthquakes.ipynb

bash install_missing.sh

./transform.py

ls -l

gsutil cp earthquakes.* gs://qwiklabs-gcp-7213f420216c9afb/earthquakes/

gsutil acl ch -u AllUsers:R gs://qwiklabs-gcp-7213f420216c9afb/earthquakes/*



==================================
Big Query

qwiklabs-gcp-4503425b33f0d032

gcloud compute zones list

datalab create mydatalabvm --zone us-east1-c

- Click on Web Preview icon on the top-right corner of the Cloud Shell ribbon. Click Change Port and enter the port 8081 and click Change and Preview.

- https://console.cloud.google.com/apis/library/sourcerepo.googleapis.com/?q=Repositories

- https://8081-dot-7202347-dot-devshell.appspot.com/tree/datalab

#standardSQL
SELECT
  departure_delay,
  COUNT(1) AS num_flights,
  APPROX_QUANTILES(arrival_delay, 5) AS arrival_delay_quantiles
FROM
  `bigquery-samples.airline_ontime_data.flights`
GROUP BY
  departure_delay
HAVING
  num_flights > 100
ORDER BY

#standardSQL
SELECT
  departure_airport,
  arrival_airport,
  COUNT(1) AS num_flights
FROM
  `bigquery-samples.airline_ontime_data.flights`
GROUP BY
  departure_airport,
  arrival_airport
ORDER BY
  num_flights DESC
LIMIT
  10
  departure_delay ASC


query="""
SELECT
  departure_delay,
  COUNT(1) AS num_flights,
  APPROX_QUANTILES(arrival_delay, 10) AS arrival_delay_deciles
FROM
  `bigquery-samples.airline_ontime_data.flights`
GROUP BY
  departure_delay
HAVING
  num_flights > 100
ORDER BY
  departure_delay ASC
"""

import google.datalab.bigquery as bq
df = bq.Query(query).execute().result().to_dataframe()
df.head()


import pandas as pd
percentiles = df['arrival_delay_deciles'].apply(pd.Series)
percentiles = percentiles.rename(columns = lambda x : str(x*10) + "%")
df = pd.concat([df['departure_delay'], percentiles], axis=1)
df.head()


without_extremes = df.drop(['0%', '100%'], 1)
without_extremes.plot(x='departure_delay', xlim=(-30,50), ylim=(-50,50));


datalab connect mydatalabvm

===================================================================
Invoking Machine Learning APIs

- 8081
- repocheckout

%bash
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
rm -rf training-data-analyst/.git

- ensure you see the training-data-analyst

- In the Datalab browser, navigate to training-data-analyst > courses > machine_learning > deepdive > 01_googleml > mlapis.ipynb

===================================================================
Lab: Creating Repeatable Dataset Splits in BigQuery & Lab: Exploring and Creating ML Datasets

- 8081

in notebook
- Rename this notebook as repocheckout

%bash
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
rm -rf training-data-analyst/.git

- datalab > notebooks > training-data-analyst > courses > machine_learning > deepdive > 02_generalization and open repeatable_splitting.ipynb

- (if disconnected) datalab connect mydatalabvm

query = """
SELECT
  pickup_datetime,
  pickup_longitude, pickup_latitude, 
  dropoff_longitude, dropoff_latitude,
  passenger_count,
  trip_distance,
  tolls_amount,
  fare_amount,
  total_amount
FROM 
  `nyc-tlc.yellow.trips`
WHERE
  MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 100000) = 0
"""
trips = bq.Query(query).execute().result().to_dataframe()


[Course 3 - Intro to TensorFlow] ===================================================================
Lab: [ML on GCP C3] Implementing a Machine Learning model in TensorFlow using Estimator API

- Rename this notebook as repocheckout.
%bash
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
rm -rf training-data-analyst/.git

- In Cloud Datalab, click on the Home icon, and then navigate to datalab > notebooks > training-data-analyst > courses > machine_learning > deepdive > 03_tensorflow > labs and open b_estimator.ipynb.

import tensorflow as tf
import pandas as pd
import numpy as np
import shutil

print(tf.__version__)


# In CSV, label is the first column, after the features, followed by the key
CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']
FEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]
LABEL = CSV_COLUMNS[0]

df_train = pd.read_csv('./taxi-train.csv', header = None, names = CSV_COLUMNS)
df_valid = pd.read_csv('./taxi-valid.csv', header = None, names = CSV_COLUMNS)
df_test = pd.read_csv('./taxi-test.csv', header = None, names = CSV_COLUMNS)


def make_train_input_fn(df, num_epochs):
  return tf.estimator.inputs.pandas_input_fn(
    x = df,
    y = df[LABEL],
    batch_size = 128,
    num_epochs = num_epochs,
    shuffle = True,
    queue_capacity = 1000
  )
  
  
def make_eval_input_fn(df):
  return tf.estimator.inputs.pandas_input_fn(
    x = df,
    y = df[LABEL],
    batch_size = 128,
    shuffle = True,
    queue_capacity = 1000
  )


def make_prediction_input_fn(df):
  return tf.estimator.inputs.pandas_input_fn(
    x = df,
    y = None,
    batch_size = 128,
    shuffle = True,
    queue_capacity = 1000
  )
  

def create_feature_cols():
  return [tf.feature_column.numeric_column(k) for k in FEATURES]


tf.logging.set_verbosity(tf.logging.INFO)

OUTDIR = 'taxi_trained'
shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time

# TODO: Train a linear regression model
model = tf.estimator.LinearRegressor(feature_columns = create_feature_cols(), model_dir = OUTDIR)

model.train(input_fn = make_train_input_fn(df_train, num_epochs = 10))


def print_rmse(model, df):
  metrics = model.evaluate(input_fn = make_eval_input_fn(df))
  print('RMSE on dataset = {}'.format(np.sqrt(metrics['average_loss'])))
print_rmse(model, df_valid)


predictions = model.predict(input_fn = make_prediction_input_fn(df_test))
for p in predictions:
  print(p)


tf.estimator.DNNClassifier(feature_columns = create_feature_cols(), hidden_units = [32, 8, 2], model_dir=OUTDIR)
model.train(input_fn = make_train_input_fn(df_train, num_epochs = 50))


import google.datalab.bigquery as bq
import numpy as np
import pandas as pd

def create_query(phase, EVERY_N):
  """
  phase: 1 = train 2 = valid
  """
  base_query = """
SELECT
  (tolls_amount + fare_amount) AS fare_amount,
  EXTRACT(DAYOFWEEK FROM pickup_datetime) * 1.0 AS dayofweek,
  EXTRACT(HOUR FROM pickup_datetime) * 1.0 AS hourofday,
  pickup_longitude AS pickuplon,
  pickup_latitude AS pickuplat,
  dropoff_longitude AS dropofflon,
  dropoff_latitude AS dropofflat,
  passenger_count * 1.0 AS passengers,
  CONCAT(CAST(pickup_datetime AS STRING), CAST(pickup_longitude AS STRING), CAST(pickup_latitude AS STRING), CAST(dropoff_latitude AS STRING), CAST(dropoff_longitude AS STRING)) AS key
FROM
  `nyc-tlc.yellow.trips`
WHERE
  trip_distance > 0
  AND fare_amount >= 2.5
  AND pickup_longitude > -78
  AND pickup_longitude < -70
  AND dropoff_longitude > -78
  AND dropoff_longitude < -70
  AND pickup_latitude > 37
  AND pickup_latitude < 45
  AND dropoff_latitude > 37
  AND dropoff_latitude < 45
  AND passenger_count > 0
  """

  if EVERY_N == None:
    if phase < 2:
      # Training
      query = "{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 4) < 2".format(base_query)
    else:
      # Validation
      query = "{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 4) = {1}".format(base_query, phase)
  else:
    query = "{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), {1}) = {2}".format(base_query, EVERY_N, phase)
    
  return query

query = create_query(2, 100000)
df = bq.Query(query).execute().result().to_dataframe()


==================================
Lab: Creating a distributed training TensorFlow model with Estimator API

- In Cloud Datalab, click on the Home icon, and then navigate to datalab > notebooks > training-data-analyst > courses > machine_learning > deepdive > 03_tensorflow > labs and open d_traineval.ipynb.

import datalab.bigquery as bq
import tensorflow as tf
import numpy as np
import shutil
from google.datalab.ml import TensorBoard
print(tf.__version__)


CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']
LABEL_COLUMN = 'fare_amount'
DEFAULTS = [[0.0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]

def read_dataset(filename, mode, batch_size = 512):
  def decode_csv(value_column):
    columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)
    features = dict(zip(CSV_COLUMNS, columns))
    label = features.pop(LABEL_COLUMN)
    return features, label

  # Create list of file names that match "glob" pattern (i.e. data_file_*.csv)
  filenames_dataset = tf.data.Dataset.list_files(filename)
  # Read lines from text files
  textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)
  # Parse text lines as comma-separated values (CSV)
  dataset = textlines_dataset.map(decode_csv)

  # Note:
  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)
  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)

  if mode == tf.estimator.ModeKeys.TRAIN:
      num_epochs = None # indefinitely
      dataset = dataset.shuffle(buffer_size = 10 * batch_size)
  else:
      num_epochs = 1 # end-of-input after this

  dataset = dataset.repeat(num_epochs).batch(batch_size)

  return dataset


INPUT_COLUMNS = [
    tf.feature_column.numeric_column('pickuplon'),
    tf.feature_column.numeric_column('pickuplat'),
    tf.feature_column.numeric_column('dropofflat'),
    tf.feature_column.numeric_column('dropofflon'),
    tf.feature_column.numeric_column('passengers'),
]

def add_more_features(feats):
  # Nothing to add (yet!)
  return feats

feature_cols = add_more_features(INPUT_COLUMNS)


## TODO: Create serving input function
def serving_input_fn():
  #ADD CODE HERE
  json_feature_placeholders = {
    'pickuplon': tf.placeholder(tf.float32, [None]),
    'pickuplat': tf.placeholder(tf.float32, [None]),
    'dropofflat': tf.placeholder(tf.float32, [None]),
    'dropofflon': tf.placeholder(tf.float32, [None]),
    'passengers': tf.placeholder(tf.float32, [None])
  }
  features = json_feature_placeholders
  return tf.estimator.export.ServingInputReceiver(features, json_feature_placeholders)


## TODO: Create train and evaluate function using tf.estimator
def train_and_evaluate(output_dir, num_train_steps):
  #ADD CODE HERE
  estimator = tf.estimator.LinearRegressor(model_dir=output_dir, feature_columns=feature_cols)
  
  train_spec = tf.estimator.TrainSpec(
      input_fn=lambda: read_dataset('./taxi-train.csv', mode=tf.estimator.ModeKeys.TRAIN),
      max_steps = num_train_steps)
  
  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)
  
  eval_spec = tf.estimator.EvalSpec(
      input_fn = lambda: read_dataset('./taxi-valid.csv', mode = tf.estimator.ModeKeys.EVAL),
      steps = None,
      exporters = exporter)
  
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)


OUTDIR = 'taxi_trained'
TensorBoard().start(OUTDIR)


# Run training    
shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time
tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file
train_and_evaluate(OUTDIR, num_train_steps = 2000)


# to list Tensorboard instances
TensorBoard().list()


# to stop TensorBoard fill the correct pid below
TensorBoard().stop(27855)
print("Stopped Tensorboard")

